{
  "metadata": {
    "kernelspec": {
      "name": "python",
      "display_name": "Python (Pyodide)",
      "language": "python"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "python",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8"
    }
  },
  "nbformat_minor": 4,
  "nbformat": 4,
  "cells": [
    {
      "cell_type": "code",
      "source": "# Q1. What is an ensemble technique in machine learning?\n\nAn ensemble technique combines multiple individual models to produce a stronger and more accurate predictive model. The idea is to aggregate the outputs of several models to \nimprove the overall performance.\n\n# Q2. Why are ensemble techniques used in machine learning?\n    \nEnsemble techniques are used to:\nIncrease model accuracy.\nReduce overfitting by combining various models with different strengths.\nLeverage diversity among individual models to improve predictions.\n\n# Q3. What is bagging?\nBagging (Bootstrap Aggregating) is an ensemble technique where multiple models (typically the same type) are trained on different random subsets of the data, obtained by bootstrapping\n(sampling with replacement). The final prediction is made by averaging (for regression) or voting (for classification) the predictions of all models.\n\n# Q4. What is boosting?\nBoosting is an ensemble technique that combines multiple weak learners (typically decision trees) into a strong model by iteratively training models on the residual errors of previous \nmodels. Each new model focuses on the errors made by the previous ones.\n\n# Q5. What are the benefits of using ensemble techniques?\n    \nImproved Accuracy: Combining multiple models often results in better predictive performance than individual models.\nReduced Overfitting: Averaging or voting reduces the risk of overfitting to the training data.\nRobustness: Ensemble methods are less sensitive to noisy data compared to single models.\n\n# Q6. Are ensemble techniques always better than individual models?\nNo, ensemble techniques are not always better. They can be computationally expensive, and if the individual models are poor, combining them may not improve performance.\nThey work best when the base models are strong and diverse.\n\n# Q7. How is the confidence interval calculated using bootstrap?\n\nResample the data with replacement to generate multiple bootstrap samples.\nCalculate the statistic of interest (e.g., mean) for each bootstrap sample.\nCreate a distribution of the statistic from all the bootstrap samples.\nCalculate the percentiles (e.g., 2.5th and 97.5th) of the distribution to form the confidence interval.\n\n# Q8. How does bootstrap work and What are the steps involved in bootstrap?\n\nBootstrap involves the following steps:\nRandom sampling with replacement from the original dataset to create multiple bootstrap samples.\nFor each bootstrap sample, calculate the statistic of interest (mean, median, etc.).\nRepeat the process many times (e.g., 1000 iterations).\nAnalyze the distribution of the statistics to estimate the confidence interval or other properties.\n",
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": "# Q9. A researcher wants to estimate the mean height of a population of trees. They measure the height of a sample of 50 trees and obtain a mean height of 15 meters and a standard \n#     deviation of 2 meters. Use bootstrap to estimate the 95% confidence interval for the population mean height.\n\nimport numpy as np\n\nsample_mean = 15\nsample_std = 2\nn = 50\nn_bootstrap = 1000\n\noriginal_sample = np.random.normal(sample_mean, sample_std, n)\nbootstrap_means = []\nfor _ in range(n_bootstrap):\n    bootstrap_sample = np.random.choice(original_sample, size=n, replace=True)\n    bootstrap_means.append(np.mean(bootstrap_sample))\n    \nlower = np.percentile(bootstrap_means, 2.5)\nupper = np.percentile(bootstrap_means, 97.5)\n(lower, upper)\n\n\n# The 95% confidence interval for the population mean height is between the lower and upper percentiles of the bootstrap sample means.",
      "metadata": {
        "trusted": true
      },
      "outputs": [
        {
          "execution_count": 1,
          "output_type": "execute_result",
          "data": {
            "text/plain": "(14.89082237823544, 15.86839292665062)"
          },
          "metadata": {}
        }
      ],
      "execution_count": 1
    }
  ]
}